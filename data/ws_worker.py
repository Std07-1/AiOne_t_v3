"""Binance Futures WebSocket –≤–æ—Ä–∫–µ—Ä ‚Üí UnifiedDataStore / Redis.

–®–ª—è—Ö: ``data/ws_worker.py``

–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:
    ‚Ä¢ –æ—Ç—Ä–∏–º—É—î –ø–æ—Ç—ñ–∫ 1m kline —á–µ—Ä–µ–∑ Binance Futures WS;
    ‚Ä¢ –æ–Ω–æ–≤–ª—é—î —Ö–≤–∏–ª–∏–Ω–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é (legacy blob –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑—á–∏—Ç—É–≤–∞–Ω–Ω—è + UnifiedDataStore.put_bars);
    ‚Ä¢ –ø—Ä–∏ —Ñ—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ö–≤–∏–ª–∏–Ω–∏ –∞–≥—Ä–µ–≥—É—î 1h –±–∞—Ä —ñ –ø—É–±–ª—ñ–∫—É—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è (``klines.1h.update``);
    ‚Ä¢ –ø—ñ–¥—Ç—Ä–∏–º—É—î –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π whitelist —Å–∏–º–≤–æ–ª—ñ–≤ (prefilter —á–µ—Ä–µ–∑ Redis —Å–µ–ª–µ–∫—Ç–æ—Ä).

–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
    ‚Ä¢ reconnect —ñ–∑ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–º backoff;
    ‚Ä¢ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏–π refresh —Å–∏–º–≤–æ–ª—ñ–≤ –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ reconnect (UNSUBSCRIBE/SUBSCRIBE);
    ‚Ä¢ fallback –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ —Å–∏–º–≤–æ–ª–∏, —è–∫—â–æ —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ—Ä–æ–∂–Ω—ñ–π —á–∏ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π.
"""

import asyncio
import json
import logging
import os
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd
import websockets
from rich.console import Console
from rich.logging import RichHandler
import orjson
from lz4.frame import compress, decompress


# ‚îÄ‚îÄ –í–±—É–¥–æ–≤–∞–Ω—ñ (–º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ) —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ç–æ—Ä–∏ DataFrame (–≤–∏–¥–∞–ª–µ–Ω–æ raw_data.py) ‚îÄ‚îÄ
def _df_to_bytes(df: pd.DataFrame, *, compress_lz4: bool = True) -> bytes:
    """–°–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è DataFrame ‚Üí bytes (orjson + –æ–ø—Ü—ñ–π–Ω–æ LZ4).

    –¢—ñ–ª—å–∫–∏ –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞ —ñ–Ω–¥–µ–∫—Å: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ñ–æ—Ä–º–∞—Ç orient="split".
    –Ø–∫—â–æ —î datetime –∫–æ–ª–æ–Ω–∫–∞ `timestamp`, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —É int64 ms –¥–ª—è JS.
    """
    df_out = df.copy()
    if "timestamp" in df_out.columns and pd.api.types.is_datetime64_any_dtype(
        df_out["timestamp"]
    ):
        df_out["timestamp"] = (df_out["timestamp"].astype("int64") // 1_000_000).astype(
            "int64"
        )
    raw_json = orjson.dumps(df_out.to_dict(orient="split"))
    return compress(raw_json) if compress_lz4 else raw_json


def _bytes_to_df(buf: bytes | str, *, compressed: bool = True) -> pd.DataFrame:
    """–î–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è bytes ‚Üí DataFrame (reverse _df_to_bytes)."""
    raw = buf.encode() if isinstance(buf, str) else buf
    if compressed:
        raw = decompress(raw)
    obj = orjson.loads(raw)
    df = pd.DataFrame(**obj)
    if "timestamp" in df.columns and pd.api.types.is_integer_dtype(df["timestamp"]):
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
    return df


from data.unified_store import (
    UnifiedDataStore,
)  # unified store (single source of truth)

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è/–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏ ---
PARTIAL_CHANNEL = "klines.1m.partial"
FINAL_CHANNEL = "klines.1h.update"
# Default TTLs for legacy blob snapshot (NOT the canonical ds.cfg.intervals_ttl)
DEFAULT_INTERVALS_TTL: Dict[str, int] = {"1m": 90, "1h": 65 * 60}
SELECTOR_REFRESH_S = 30  # how often to refresh symbol whitelist

STATIC_SYMBOLS = os.getenv("STREAM_SYMBOLS", "")
DEFAULT_SYMBOLS = [s.lower() for s in STATIC_SYMBOLS.split(",") if s] or ["btcusdt"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –õ–æ–≥—É–≤–∞–Ω–Ω—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logger = logging.getLogger("app.data.ws_worker")
if not logger.handlers:
    logger.setLevel(logging.WARNING)
    logger.addHandler(RichHandler(console=Console(stderr=True), show_path=False))
    logger.propagate = False

logging.getLogger("websockets").setLevel(logging.WARNING)
logging.getLogger("websockets.client.protocol").setLevel(logging.WARNING)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ WS Worker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


class WSWorker:
    """WebSocket worker streaming 1m Binance klines directly into UnifiedDataStore.

    Legacy SimpleCacheHandler / RAMBuffer are removed. Minute bars persisted via
    store.put_bars; last bar & fast symbols served from Redis through the store.
    """

    def __init__(
        self,
        symbols: Optional[List[str]] = None,
        *,
        store: UnifiedDataStore,
        selectors_key: Optional[str] = None,
        intervals_ttl: Optional[Dict[str, int]] = None,
    ):
        if store is None:
            raise ValueError("WSWorker requires a UnifiedDataStore instance")
        self.store = store
        self._symbols: Set[str] = set(
            [s.lower() for s in symbols] if symbols else DEFAULT_SYMBOLS
        )
        # optional override for selector redis key ("part1:part2:..")
        self._selectors_key: Optional[Tuple[str, ...]] = (
            tuple(p for p in selectors_key.split(":") if p) if selectors_key else None
        )
        # TTLs for legacy blob snapshots; fall back to defaults above
        mapping = DEFAULT_INTERVALS_TTL.copy()
        if intervals_ttl:
            mapping.update(intervals_ttl)
        self._ttl_1m = mapping.get("1m", DEFAULT_INTERVALS_TTL["1m"])
        self._ttl_1h = mapping.get("1h", DEFAULT_INTERVALS_TTL["1h"])
        self._ws_url: Optional[str] = None
        self._backoff: int = 3
        self._refresh_task: Optional[asyncio.Task] = None
        self._stop_event = asyncio.Event()

    async def _get_live_symbols(self) -> List[str]:
        """Fetch whitelist symbols either via custom selectors_key or store helper.

        selectors_key (if provided) is a colon-delimited path inside the store namespace
        e.g. "selectors:fast_symbols" -> ai_one:selectors:fast_symbols
        """
        if self._selectors_key:
            try:
                data = await self.store.redis.jget(*self._selectors_key, default=[])
            except Exception as e:  # pragma: no cover
                logger.warning("[WSWorker] selectors_key fetch failed: %s", e)
                data = []
        else:
            data = await self.store.get_fast_symbols()
        # –î–µ—Ñ–æ–ª—Ç ‚Äî —è–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –ø—Ä–∏–π—à–ª–æ, –∞–±–æ —Ç–∏–ø –Ω–µ–≤—ñ–¥–æ–º–∏–π
        syms = []
        if isinstance(data, dict):
            syms = list(data.keys())
        elif isinstance(data, list):
            syms = data
        # Fallback —è–∫—â–æ –ø–æ—Ä–æ–∂–Ω—ñ–π
        if not syms:
            logger.warning(
                "[WSWorker] selector:active:stream –ø—É—Å—Ç–∏–π –∞–±–æ –Ω–µ–≤–∞–ª–∏–¥–Ω–∏–π, fallback btcusdt"
            )
            syms = DEFAULT_SYMBOLS
        if len(syms) < 3:
            logger.warning(
                "[WSWorker] –í–ê–ñ–õ–ò–í–û: –ö—ñ–ª—å–∫—ñ—Å—Ç—å symbols —É —Å—Ç—Ä—ñ–º—ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ –º–∞–ª–∞: %d (%s)",
                len(syms),
                syms,
            )
        logger.debug(
            "[WSWorker][_get_live_symbols] data type: %s, value: %s",
            type(data),
            str(data)[:200],
        )
        logger.debug("[WSWorker] –°–∏–º–≤–æ–ª–∏ –¥–ª—è —Å—Ç—Ä—ñ–º—É: %d (%s...)", len(syms), syms[:10])
        return syms

    def _build_ws_url(self, symbols: Set[str]) -> str:
        streams = "/".join(f"{s}@kline_1m" for s in sorted(symbols))
        return f"wss://fstream.binance.com/stream?streams={streams}"

    async def _store_minute(self, sym: str, ts: int, k: Dict[str, Any]) -> pd.DataFrame:
        """Incrementally update 1m history snapshot blob (legacy format) for quick replay.

        This keeps backward compatibility for any code still reading the old serialized
        frame while the canonical last bar lives in store.redis (jset candles...).
        """
        raw = await self.store.fetch_from_cache(sym, "1m", prefix="candles", raw=True)
        if raw:
            try:
                df = _bytes_to_df(raw)
            except Exception:
                df = pd.DataFrame(columns=["open", "high", "low", "close", "volume"])
        else:
            df = pd.DataFrame(columns=["open", "high", "low", "close", "volume"])
        dt = pd.to_datetime(ts, unit="ms", utc=True)
        df.loc[dt] = [
            float(k["o"]),
            float(k["h"]),
            float(k["l"]),
            float(k["c"]),
            float(k["v"]),
        ]
        await self.store.store_in_cache(
            sym, "1m", _df_to_bytes(df), ttl=self._ttl_1m, prefix="candles", raw=True
        )
        return df

    async def _on_final_candle(self, sym: str, df_1m: pd.DataFrame) -> None:
        """–ê–≥—Ä–µ–≥—É—î 1h-–±–∞—Ä, –∑–±–µ—Ä—ñ–≥–∞—î —É Redis, –ø—É–±–ª—ñ–∫—É—î –ø–æ–¥—ñ—é."""
        df_1h = (
            df_1m.resample("1h", label="right", closed="right")
            .agg(
                {
                    "open": "first",
                    "high": "max",
                    "low": "min",
                    "close": "last",
                    "volume": "sum",
                }
            )
            .dropna()
        )
        await self.store.store_in_cache(
            sym, "1h", _df_to_bytes(df_1h), ttl=self._ttl_1h, prefix="candles", raw=True
        )
        await self.store.redis.r.publish(FINAL_CHANNEL, sym)
        logger.debug("[%s] 1h closed ‚Üí published %s", sym, FINAL_CHANNEL)

    async def _handle_kline(self, k: Dict[str, Any]) -> None:
        """
        –û–±—Ä–æ–±–∫–∞ WS kline:
        - –∑–±–µ—Ä—ñ–≥–∞—î 1m –≤ RAMBuffer (–æ–Ω–æ–≤–ª–µ–Ω–Ω—è bar –ø–æ timestamp, –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è),
        - –ø—Ä–∏ –∑–∞–∫—Ä–∏—Ç—Ç—ñ bar[x] ‚Äî –∞–≥—Ä–µ–≥—É—î —É 1h.
        """
        sym = k["s"].lower()
        ts = int(k["t"])
        tf = "1m"
        bar = {
            "open": float(k["o"]),
            "high": float(k["h"]),
            "low": float(k["l"]),
            "close": float(k["c"]),
            "volume": float(k["v"]),
            "timestamp": ts,
        }
        # Write via UnifiedDataStore (single row DataFrame)
        df_row = pd.DataFrame(
            [
                {
                    "open_time": ts,
                    "open": bar["open"],
                    "high": bar["high"],
                    "low": bar["low"],
                    "close": bar["close"],
                    "volume": bar["volume"],
                    "close_time": ts + 60_000,
                }
            ]
        )
        try:
            await self.store.put_bars(sym, tf, df_row)
        except Exception as e:
            logger.warning("Failed to put bars into UnifiedDataStore: %s", e)
        # –õ—ñ—á–∏–ª—å–Ω–∏–∫ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å WS
        try:
            self.store.metrics.errors.inc(stage="ws_msg")  # reuse counter namespace
        except Exception:
            pass

        # –ó–∞–ø–∏—Å —É Redis (—è–∫ fallback —ñ –¥–ª—è stage2+)
        df_1m = await self._store_minute(sym, ts, k)
        await self.store.redis.r.publish(PARTIAL_CHANNEL, sym)
        if k.get("x"):
            await self._on_final_candle(sym, df_1m)

    async def _refresh_symbols(self, ws: websockets.WebSocketClientProtocol) -> None:
        """–§–æ–Ω–æ–≤–∏–π —Ç–∞—Å–∫: refresh whitelist —ñ —Ä–µ—Å–∞–±—Å–∫—Ä–∞–π–±."""
        while not self._stop_event.is_set():
            await asyncio.sleep(SELECTOR_REFRESH_S)
            try:
                new_syms = set(await self._get_live_symbols())
                if new_syms and new_syms != self._symbols:
                    await self._resubscribe(ws, new_syms)
            except Exception as e:
                logger.warning("Refresh symbols error: %s", e)

    async def _resubscribe(
        self, ws: websockets.WebSocketClientProtocol, new_syms: Set[str]
    ) -> None:
        """UNSUBSCRIBE/SUBSCRIBE WS-–∫–∞–Ω–∞–ª–∏ –±–µ–∑ reconnect."""
        old_syms = self._symbols
        to_unsub = [f"{s}@kline_1m" for s in old_syms - new_syms]
        to_sub = [f"{s}@kline_1m" for s in new_syms - old_syms]
        rid = 1
        if to_unsub:
            await ws.send(
                json.dumps({"method": "UNSUBSCRIBE", "params": to_unsub, "id": rid})
            )
            rid += 1
        if to_sub:
            await ws.send(
                json.dumps({"method": "SUBSCRIBE", "params": to_sub, "id": rid})
            )
            logger.debug(
                "WS re-subscribed +%d -%d total=%d",
                len(to_sub),
                len(to_unsub),
                len(new_syms),
            )
        self._symbols = new_syms

    async def consume(self) -> None:
        """–ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è, –æ–±—Ä–æ–±–∫–∞ WS, reconnect/backoff."""
        while not self._stop_event.is_set():
            try:
                # 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∑–Ω–∞—á–∞—î–º–æ syms, fallback –Ω–∞ {"btcusdt"}
                syms = set(await self._get_live_symbols() or [])
                if not syms:
                    syms = {"btcusdt"}
                # 2. –û–Ω–æ–≤–ª—é—î–º–æ ws_url –ª–∏—à–µ —è–∫—â–æ —Å–∏–º–≤–æ–ª–∏ –∑–º—ñ–Ω–∏–ª–∏—Å—å
                if syms != self._symbols or not self._ws_url:
                    self._symbols = syms
                    self._ws_url = self._build_ws_url(self._symbols)

                logger.info(
                    "üîÑ –ó–∞–ø—É—Å–∫ WS (%d symbols): %s",
                    len(self._symbols),
                    list(self._symbols)[:5],
                )
                async with websockets.connect(self._ws_url, ping_interval=20) as ws:
                    logger.debug("WS connected (%d streams)‚Ä¶", len(self._symbols))
                    self._backoff = 3
                    self._stop_event.clear()
                    self._refresh_task = asyncio.create_task(self._refresh_symbols(ws))
                    async for msg in ws:
                        try:
                            data = json.loads(msg).get("data", {}).get("k")
                            if data:
                                await self._handle_kline(data)
                        except Exception as e:
                            try:
                                msg_for_log = msg
                                if isinstance(msg, dict):
                                    msg_for_log = json.dumps(msg, default=str)[:80]
                                logger.debug(
                                    "Bad WS message: %s‚Ä¶ (%s)", str(msg)[:200], e
                                )
                            except Exception as e2:
                                logger.debug("Bad WS message: <unserializable> (%s)", e)
            except Exception as exc:
                logger.warning("WS error: %s ‚Üí reconnect in %ds", exc, self._backoff)
                await asyncio.sleep(self._backoff)
                self._backoff = min(self._backoff * 2, 30)
            finally:
                if self._refresh_task:
                    self._refresh_task.cancel()

    async def stop(self) -> None:
        """–ó—É–ø–∏–Ω—è—î –≤–æ—Ä–∫–µ—Ä —ñ –≤—Å—ñ —Ñ–æ–Ω–æ–≤—ñ —Ç–∞—Å–∫–∏."""
        self._stop_event.set()
        if self._refresh_task:
            self._refresh_task.cancel()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    worker = WSWorker()
    try:
        asyncio.run(worker.consume())
    except KeyboardInterrupt:
        logger.info("WSWorker stopped by user")
    except Exception as e:
        logger.error("Fatal error: %s", e, exc_info=True)
